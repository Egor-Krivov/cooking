{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = utils.Data(test=True)\n",
    "\n",
    "ingredient_data = data.ingredient_data\n",
    "word_data = data.word_data\n",
    "tfidf_word_data = data.tfidf_word_data\n",
    "\n",
    "recipes = data.recipes\n",
    "documents = data.documents\n",
    "\n",
    "y_model = data.y_model\n",
    "y = data.y\n",
    "y_hc = data.y_hc\n",
    "test_ids = data.test_ids\n",
    "\n",
    "cv = data.cv10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model is neural network. It uses concatenated word_data and ingredient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sparse.hstack((ingredient_data, word_data)).toarray().astype(np.int8)\n",
    "x_test = sparse.hstack((data.ingredient_test, data.word_test)).toarray().astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = data.cv10\n",
    "nb_epoch = 50\n",
    "batch_size = 2048\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.5, input_shape=(x.shape[1],)))\n",
    "model.add(Dense(800, init='he_normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(400, init='he_normal', activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, init='he_normal', activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_hc.shape[1], init='he_normal', activation='softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "model_description = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('predicting for train set')\n",
    "\n",
    "predictions = []\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    model = model_from_json(model_description)\n",
    "    model.fit(x[train], y_hc[train], nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "    p = model.predict(x[test], batch_size=batch_size)\n",
    "    p.dump('nn_train_{}'.format(i))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    predictions = [np.load('nn_train_{}'.format(i)) for i in range(10)]\n",
    "\n",
    "prediction = np.empty(y_hc.shape)\n",
    "for p, (train, test) in zip(predictions, cv):\n",
    "    prediction[test] = p\n",
    "    \n",
    "np.save('nn_train', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_from_json(model_description)\n",
    "model.fit(x, y_hc, nb_epoch=nb_epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = model.predict(x_test, batch_size=batch_size)\n",
    "np.save('nn_test', test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "C = 3.1622776601683795\n",
    "clf = SVC(gamma=gamma, C=C, probability=True)\n",
    "\n",
    "data.predict_cv(tfidf_word_data, y, data.tfidf_word_test, clf, 'SVC_rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814300799517\n",
      "0.815457333937\n"
     ]
    }
   ],
   "source": [
    "data_names = ['nn', 'SVC_rbf']\n",
    "\n",
    "predictions = []\n",
    "for name in data_names:\n",
    "    name += '_train.npy'\n",
    "    res = np.load(name)\n",
    "    print(accuracy_score(y, np.argmax(res, axis=1)))\n",
    "    predictions.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82332679639965811"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, np.argmax(np.mean(np.array(predictions), axis=0), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for name in data_names:\n",
    "    name += '_test.npy'\n",
    "    p = np.load(name)\n",
    "    predictions.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.argmax(np.mean(np.array(predictions), axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submission(p, filename):\n",
    "    p = data.y_model.inverse_transform(p)\n",
    "    sub = pd.DataFrame({'cuisine': p}, index=test_ids)\n",
    "    sub.index.name='id'\n",
    "    sub.to_csv(filename)\n",
    "\n",
    "make_submission(prediction, 'submission_mean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
